{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# New added\n",
    "import pickle\n",
    "\n",
    "\n",
    "class CLIPCapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, append=False, prefix='A photo depicts'):\n",
    "        self.data = data\n",
    "        self.prefix = ''\n",
    "        if append:\n",
    "            self.prefix = prefix\n",
    "            if self.prefix[-1] != ' ':\n",
    "                self.prefix += ' '\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.data[idx]\n",
    "        c_data = clip.tokenize(self.prefix + c_data, truncate=True).squeeze()\n",
    "        return {'caption': c_data}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def Convert(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "\n",
    "class CLIPImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # only 224x224 ViT-B/32 supported for now\n",
    "        self.preprocess = self._transform_test(224)\n",
    "\n",
    "    def _transform_test(self, n_px):\n",
    "        return Compose([\n",
    "            Resize(n_px, interpolation=Image.BICUBIC),\n",
    "            CenterCrop(n_px),\n",
    "            Convert,\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                      (0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.data[idx]\n",
    "        if isinstance(c_data, str):\n",
    "            image = Image.open(c_data)\n",
    "        elif isinstance(c_data, Image.Image):\n",
    "            image = c_data\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image type: {type(c_data)}\")\n",
    "        image = self.preprocess(image)\n",
    "        return {'image': image}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class DINOImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # only 224x224 ViT-B/32 supported for now\n",
    "        self.preprocess = self._transform_test(224)\n",
    "\n",
    "    def _transform_test(self, n_px):\n",
    "        return Compose([\n",
    "            Resize(256, interpolation=Image.BICUBIC),\n",
    "            CenterCrop(n_px),\n",
    "            Convert,\n",
    "            ToTensor(),\n",
    "            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_data = self.data[idx]\n",
    "        # 支持路径或PIL.Image对象\n",
    "        if isinstance(c_data, str):\n",
    "            image = Image.open(c_data)\n",
    "        elif isinstance(c_data, Image.Image):\n",
    "            image = c_data\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image type: {type(c_data)}\")\n",
    "        image = self.preprocess(image)\n",
    "        return {'image': image}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def extract_all_captions(captions, model, device, batch_size=256, num_workers=8, append=False):\n",
    "    data = torch.utils.data.DataLoader(\n",
    "        CLIPCapDataset(captions, append=append),\n",
    "        batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    all_text_features = []\n",
    "    with torch.no_grad():\n",
    "        # for b in tqdm(data):\n",
    "        for b in data:\n",
    "            b = b['caption'].to(device)\n",
    "            all_text_features.append(model.encode_text(b).cpu().numpy())\n",
    "    all_text_features = np.vstack(all_text_features)\n",
    "    return all_text_features\n",
    "\n",
    "\n",
    "def extract_all_images(images, model, datasetclass, device, batch_size=1, num_workers=1):\n",
    "    data = torch.utils.data.DataLoader(\n",
    "        datasetclass(images),\n",
    "        batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    all_image_features = []\n",
    "    with torch.no_grad():\n",
    "        # for b in tqdm(data):\n",
    "        for b in data:\n",
    "            b = b['image'].to(device)\n",
    "            if hasattr(model, 'encode_image'):\n",
    "                if device == 'cuda':\n",
    "                    b = b.to(torch.float16)\n",
    "                all_image_features.append(model.encode_image(b).cpu().numpy())\n",
    "            else:\n",
    "                all_image_features.append(model(b).cpu().numpy())\n",
    "    all_image_features = np.vstack(all_image_features)\n",
    "    return all_image_features\n",
    "\n",
    "\n",
    "def get_clip_score(model, images, candidates, device, append=False, w=1.0):\n",
    "    '''\n",
    "    get standard image-text clipscore.\n",
    "    images can either be:\n",
    "    - a list of strings specifying filepaths for images\n",
    "    - a precomputed, ordered matrix of image features\n",
    "    '''\n",
    "    if isinstance(images, list):\n",
    "        # need to extract image features\n",
    "        images = extract_all_images(images, model, device)\n",
    "\n",
    "    candidates = extract_all_captions(candidates, model, device, append=append)\n",
    "\n",
    "    if version.parse(np.__version__) < version.parse('1.21'):\n",
    "        images = sklearn.preprocessing.normalize(images, axis=1)\n",
    "        candidates = sklearn.preprocessing.normalize(candidates, axis=1)\n",
    "    else:\n",
    "        images = images / np.sqrt(np.sum(images ** 2, axis=1, keepdims=True))\n",
    "        candidates = candidates / \\\n",
    "            np.sqrt(np.sum(candidates ** 2, axis=1, keepdims=True))\n",
    "\n",
    "    per = w * np.clip(np.sum(images * candidates, axis=1), 0, None)\n",
    "    return np.mean(per), per\n",
    "\n",
    "\n",
    "def clipeval(image_paths, prompts, model, device):\n",
    "    image_feats = extract_all_images(\n",
    "        image_paths, model, CLIPImageDataset, device, batch_size=1, num_workers=1)\n",
    "\n",
    "    _, per_instance_image_text = get_clip_score(\n",
    "        model, image_feats, prompts, device)\n",
    "\n",
    "    # scores = {image_path: {'CLIPScore': float(clipscore)}\n",
    "    #           for image_path, clipscore in\n",
    "    #           zip(image_paths, per_instance_image_text)}\n",
    "    scores = [float(clipscore) for clipscore in per_instance_image_text]\n",
    "\n",
    "    # return np.mean(scores), np.std([s['CLIPScore'] for s in scores.values()])\n",
    "    return np.mean(scores)\n",
    "\n",
    "def clipeval_image(image_paths, image_paths_ref, model, device):\n",
    "    image_feats = extract_all_images(\n",
    "        image_paths, model, CLIPImageDataset, device, batch_size=1, num_workers=1)\n",
    "    image_feats_ref = extract_all_images(\n",
    "        image_paths_ref, model, CLIPImageDataset, device, batch_size=1, num_workers=1)\n",
    "    image_feats = image_feats / \\\n",
    "        np.sqrt(np.sum(image_feats ** 2, axis=1, keepdims=True))\n",
    "    image_feats_ref = image_feats_ref / \\\n",
    "        np.sqrt(np.sum(image_feats_ref ** 2, axis=1, keepdims=True))\n",
    "    res = image_feats @ image_feats_ref.T\n",
    "    return np.mean(res)\n",
    "\n",
    "def dinoeval_image(image_paths, image_paths_ref, model, device):\n",
    "    image_feats = extract_all_images(\n",
    "        image_paths, model, DINOImageDataset, device, batch_size=1, num_workers=1)\n",
    "\n",
    "    image_feats_ref = extract_all_images(\n",
    "        image_paths_ref, model, DINOImageDataset, device, batch_size=1, num_workers=1)\n",
    "\n",
    "    image_feats = image_feats / \\\n",
    "        np.sqrt(np.sum(image_feats ** 2, axis=1, keepdims=True))\n",
    "    image_feats_ref = image_feats_ref / \\\n",
    "        np.sqrt(np.sum(image_feats_ref ** 2, axis=1, keepdims=True))\n",
    "    res = image_feats @ image_feats_ref.T\n",
    "    return np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import json\n",
    "from vision_transformer import vit_small\n",
    "\n",
    "device = 'cuda'\n",
    "clip_model, _ = clip.load(\"/data/oss_bucket_0/ziwei/checkpoints/ViT-B-32.pt\", device=device, jit=False)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_model = vit_small(patch_size=16)\n",
    "checkpoint = torch.load(\"/home/zw.hzw/checkpoints/dino-vits16/dino_deitsmall16_pretrain_full_checkpoint.pth\", map_location=device)\n",
    "state_dict = checkpoint[\"student\"]\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k,v in state_dict.items():\n",
    "    new_key = k.replace('module.','')\n",
    "    new_state_dict[new_key]=v\n",
    "\n",
    "dino_model.load_state_dict(new_state_dict, strict=False)\n",
    "dino_model = dino_model.to(device)\n",
    "dino_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27392578125\n",
      "yes\n",
      "0.99803245\n",
      "yes\n",
      "0.968\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "ref_img = Image.open(\"/home/zw.hzw/model_list/UNO-main/eval/test_images/121.png\")\n",
    "gen_img = Image.open(\"/home/zw.hzw/model_list/UNO-main/eval/test_images/121_1.png\")\n",
    "prompt = \"a purple can\"\n",
    "score = clipeval([ref_img], [prompt], clip_model, device)\n",
    "print(score)\n",
    "if isinstance(float(score), float):\n",
    "    print(\"yes\")\n",
    "score = dinoeval_image([gen_img], [\"/home/zw.hzw/model_list/UNO-main/eval/test_images/121.png\"], dino_model, device)\n",
    "print(score)\n",
    "if isinstance(float(score), float):\n",
    "    print(\"yes\")\n",
    "\n",
    "score = clipeval_image([gen_img], [ref_img], clip_model, device)\n",
    "print(score)\n",
    "if isinstance(float(score), float):\n",
    "    print(\"yes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "download",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
